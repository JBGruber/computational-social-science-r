---
title: "Exercises Day 2 - Obtaining data"
format: html
---

1. Consider the toy HTML example below. Which selectors do you need to put into `html_elements()` (which extracts all elements matching the selector) to extract the information


```{r}
#| eval: false
library(rvest)
webpage <- "<html>
<body>
  <h1>Computational Research in the Post-API Age</h1>
  <div class='author'>Deen Freelon</div>
  <div>Keywords:
    <ul>
      <li>API</li>
      <li>computational</li>
      <li>Facebook</li>
    </ul>
  </div>
  <div class='text'>
    <p>Three pieces of advice on whether and how to scrape from Dan Freelon</p>
  </div>
  
  <ol class='advice'>
    <li id='one'> use authorized methods whenever possible </li>
    <li id='two'> do not confuse terms of service compliance with data protection </li>
    <li id='three'> understand the risks of violating terms of service </li>
  </ol>

</body>
</html>" |> 
  read_html()
```

```{r}
#| eval: false
# the headline
headline <- html_elements(webpage, "")
headline
# the author
author <- html_elements(webpage, "")
author
# the ordered list
ordered_list <- html_elements(webpage, "")
ordered_list
# all bullet points
bullet_points <- html_elements(webpage, "")
bullet_points
# bullet points in unordered list
bullet_points_unordered <- html_elements(webpage, "")
bullet_points_unordered
# elements in ordered list
bullet_points_ordered <- html_elements(webpage, "")
bullet_points_ordered
# third bullet point in ordered list
bullet_point_three_ordered <- html_elements(webpage, "")
bullet_point_three_ordered
```

2. Get the table with 2024 opinion polling for the United Kingdom general election from this site: <https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_2024_United_Kingdom_general_election&oldid=1232286478>

```{r}
library(tidyverse)
library(rvest)
# 1. Request & collect raw html
html <- read_html("https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_2024_United_Kingdom_general_election&oldid=1232286478")

# 2. Parse
opinion_table <- html |>
  html_elements(".wikitable") |> 
  html_table() |>                
  pluck(1)                    
```


3. Wrangle and plot the data opinion polls

```{r}
# 3. Wrangle
opinion_tidy <- opinion_table |> 
  # remove first row
  slice(-1) |> 
  # transform to a long format
  pivot_longer(cols = Con:Others, names_to = "party", values_to = "result") |> 
  # remove other parties, as the table is not clear here
  filter(party != "Others") |> 
  # Extract percentages and dates from character columns
  mutate(result_pct = as.numeric(str_extract(result, "\\d+(?=%)")),
         date_clean = str_extract(Datesconducted, "\\d{1,2} [A-z]{3}"), # this grabs the first date only, which is enough
         date = lubridate::dmy(paste(date_clean, 2024))) |> 
  filter(!is.na(result_pct),
         # some dates are parsed wrong and are in the future
         date < "2024-07-03")

# Plot!
opinion_tidy |> 
  ggplot(aes(x = date, y = result_pct, colour = party)) +
  geom_line()
```

4. For extracting text, `rvest` has two functions: `html_text` and `html_text2`. Explain the difference. You can test your explanation with the example html below.

```{r}
html <- "<p>This is some text
         some more text</p><p>A new paragraph!</p>
         <p>Quick Question, is web scraping:

         a) fun
         b) tedious
         c) I'm not sure yet!</p>" |> 
  read_html()
```

- You should almost always use `html_text2`, as the help page says (`?html_text2`)
- `html_text2`, put briefly, extracts text as the browser would display it
- `html_text`, retains the original line breaks and extra space in the raw html content, which is not what the users sees in the browser and so we are usually not interested in them
- You can easily test that with:

```{r}
html_text(html)
html_text2(html)
```

The superfluous line breaks and extra space from the html code is not displayed

5. How could you convert the `links` object below so that it contains actual URLs?

```{r}
# links collected from https://en.wikipedia.org
links <- c(
  "/wiki/2024_United_Kingdom_general_election",
  "/wiki/Main_Page",
  "/wiki/HTML_element"
)
```

What you see here are relative links, which are often used by websites.
They only work after adding the domain to the hyperlinks as well.
In R there are a few different ways to do that:

```{r}
# base R
paste("https://en.wikipedia.org", links, sep = "")

# shorthand function for sep = "" setting
paste0("https://en.wikipedia.org", links)

# tidyverse/stringr alternative
str_c("https://en.wikipedia.org", links)

# glue alternative, which allows in-string substitution with an R object
glue::glue("https://en.wikipedia.org{links}")
```

Note that you do not have to repeat the base URL, since all functions recycle "https://en.wikipedia.org" as often as there are instances in `links`. 
